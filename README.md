# ğŸ” Retrieval-Augmented Generation (RAG) System from Scratch

An end-to-end **Retrieval-Augmented Generation (RAG)** system built from first principles, demonstrating how modern AI applications combine **semantic search** with **large language models** to produce **grounded, reliable answers with citations**.

This project intentionally avoids high-level frameworks at first and implements the core mechanics manually to build **deep system-level understanding** of real-world LLM systems.

---

## ğŸš€ What This Project Does

Given a natural-language query, the system:

1. **Retrieves** the most relevant document chunks using dense vector similarity (FAISS)
2. **Augments** the user prompt with retrieved context
3. **Generates** a grounded answer using an LLM
4. **Validates** the output against a strict JSON schema
5. **Refuses** to answer when information is missing
6. **Cites** the exact document chunks used

> This is **real RAG** â€” not just semantic search and not just generation.

---

## ğŸ§  Why This Project Matters

Modern LLM applications fail not because models are weak, but because:

- Context is poorly retrieved  
- Hallucinations go unchecked  
- Outputs arenâ€™t validated  
- Cost and failure modes are ignored  

This project focuses on **engineering reliability**, not model hype.

---

## ğŸ—ï¸ System Architecture

# ğŸ” Retrieval-Augmented Generation (RAG) System from Scratch

An end-to-end **Retrieval-Augmented Generation (RAG)** system built from first principles, demonstrating how modern AI applications combine **semantic search** with **large language models** to produce **grounded, reliable answers with citations**.

This project intentionally avoids high-level frameworks at first and implements the core mechanics manually to build **deep system-level understanding** of real-world LLM systems.

---

## ğŸš€ What This Project Does

Given a natural-language query, the system:

1. **Retrieves** the most relevant document chunks using dense vector similarity (FAISS)
2. **Augments** the user prompt with retrieved context
3. **Generates** a grounded answer using an LLM
4. **Validates** the output against a strict JSON schema
5. **Refuses** to answer when information is missing
6. **Cites** the exact document chunks used

> This is **real RAG** â€” not just semantic search and not just generation.

---

## ğŸ§  Why This Project Matters

Modern LLM applications fail not because models are weak, but because:

- Context is poorly retrieved  
- Hallucinations go unchecked  
- Outputs arenâ€™t validated  
- Cost and failure modes are ignored  

This project focuses on **engineering reliability**, not model hype.

---

## ğŸ—ï¸ System Architecture

Documents
â†“
Chunking
â†“
Embeddings (Sentence Transformers)
â†“
FAISS Vector Index
â†“
Query Embedding
â†“
Top-K Retrieval
â†“
Prompt Assembly (with guardrails)
â†“
LLM Generation (OpenAI)
â†“
Schema Validation + Refusal Logic


---

## ğŸ“ Project Structure

semantic-search-Engine/
â”‚
â”œâ”€â”€ data/ # Raw text documents
â”‚
â”œâ”€â”€ chunking/
â”‚ â”œâ”€â”€ fixed.py
â”‚ â”œâ”€â”€ overlap.py
â”‚ â””â”€â”€ recursive.py
â”‚
â”œâ”€â”€ embeddings/
â”‚ â””â”€â”€ embedder.py # Sentence-transformer embedder
â”‚
â”œâ”€â”€ index/
â”‚ â”œâ”€â”€ build_index.py # Build FAISS index + metadata
â”‚ â””â”€â”€ search.py # FAISS retrieval logic
â”‚
â”œâ”€â”€ rag/
â”‚ â”œâ”€â”€ prompt_builder.py # RAG prompt + guardrails
â”‚ â”œâ”€â”€ rag_pipeline.py # Retrieval + generation
â”‚ â””â”€â”€ test_rag.py # End-to-end demo
â”‚
â”œâ”€â”€ evaluation/
â”‚ â””â”€â”€ gold_queries.json # Gold set for evaluation
â”‚
â”œâ”€â”€ index/
â”‚ â”œâ”€â”€ chunk_index.faiss
â”‚ â””â”€â”€ chunk_metadata.json
â”‚
â”œâ”€â”€ .env # OPENAI_API_KEY (gitignored)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ”‘ Key Concepts Implemented

### 1ï¸âƒ£ Semantic Search (No LLM Involved)
- Sentence-level embeddings  
- Vector normalization  
- Cosine similarity  
- FAISS indexing  
- Top-K nearest-neighbor retrieval  

---

### 2ï¸âƒ£ Chunking Strategies
- Fixed-size chunking  
- Overlapping windows  
- Structure-aware (recursive) chunking  
- Tradeoffs between **recall, precision, and cost**

---

### 3ï¸âƒ£ Retrieval-Augmented Generation (RAG)
- Context injection into prompts  
- Explicit grounding rules  
- Source citation tracking  
- Refusal behavior when context is insufficient  

---

### 4ï¸âƒ£ Prompt Engineering (Engineering-Grade)
- System-level instruction dominance  
- Explicit **â€œdonâ€™t guessâ€** rules  
- Guardrails against hallucination  
- Context-only answering  
- Confidence calibration  

---

### 5ï¸âƒ£ Output Validation
- Strict JSON schema  
- Automatic retries on invalid output  
- Fail-fast behavior after repeated violations  

---

## ğŸ§ª Example Queries & Behavior

### âœ… Grounded Answer

**Query:** *What is a vector database?*

```json
{
  "answer": "A vector database is a data store specialized for handling data represented as high-dimensional vectors, enabling efficient similarity search over embeddings.",
  "confidence": 1.0,
  "used_sources": [
    "vector_databases.txt chunk=1",
    "vector_databases.txt chunk=4"
  ]
}


ğŸš« Refusal (Correct Behavior)

Query: Who won the 2035 Cricket World Cup?

{
  "answer": "I don't know",
  "confidence": 0.3,
  "used_sources": []
}
```

This refusal is intentional and correct â€” no hallucination.

ğŸ“Š Evaluation Results

The semantic retrieval layer was evaluated using a gold query set, measuring document hit@3.

Doc Hit@3: 5 / 10 = 50%

Key Insight

Abstract conceptual queries often require LLM-based synthesis beyond pure embedding retrieval.

This motivates:

Query rewriting

Improved chunking

Hybrid retrieval strategies

ğŸ§  Engineering Lessons Learned

Embeddings â‰  answers â€” retrieval quality dominates RAG performance

Chunking strategy often matters more than model choice

Guardrails matter more than clever prompts

LLMs must be treated as unreliable collaborators

Environment setup is a real engineering challenge (Conda isolation used)

ğŸ› ï¸ Tech Stack

Python 3.10

Sentence Transformers

FAISS (CPU)

OpenAI API

Pydantic

Conda (isolated environment)

âš™ï¸ How to Run
1ï¸âƒ£ Build the index (one-time)
python -m index.build_index

2ï¸âƒ£ Run the RAG demo
python -m rag.test_rag

ğŸ”’ Security & Best Practices

API keys stored in .env

.env is gitignored

No secrets committed

Deterministic failure handling

ğŸ§­ Roadmap / Next Improvements

Query rewriting for better retrieval

RAG-specific evaluation metrics (faithfulness, groundedness)

Confidence calibration

Hybrid lexical + dense retrieval

Framework comparison (LangChain / LlamaIndex)

Agentic retrieval planning

ğŸ§‘â€ğŸ’» Author Notes

This project was built as part of a deliberate transition from ML theory â†’ AI engineering, focusing on:

Systems thinking


## FLOW OF THE PROJECT:


The full flow of your project (end-to-end, very detailed)

Iâ€™ll describe two flows:

A) Offline flow (indexing) â€” done once per corpus update
B) Online flow (query answering) â€” per user query
A) OFFLINE FLOW â€” Build the index

You run:

python -m index.build_index

Step A1 â€” Load documents from data/

reads each .txt

stores:

doc_id = filename

text = file content

Step A2 â€” Chunk each document

For each doc:

apply chosen chunker (fixed/overlap/recursive)

output many chunks:

each chunk has:

doc_id

chunk_id

text

Step A3 â€” Embed each chunk (one-time)

call your Embedder.embed(chunks_texts)

output an array:

shape: (num_chunks, embedding_dim)

Step A4 â€” Normalize embeddings (if used)

for cosine similarity you typically normalize vectors to unit length

you already do this in your Embedder

Step A5 â€” Build FAISS index

create FAISS index (usually inner product for normalized vectors)

add all chunk vectors to index

Step A6 â€” Save to disk

save index:

index/chunk_index.faiss

save metadata mapping:

index/chunk_metadata.json

maps row i â†’ {doc_id, chunk_id, text}

âœ… Offline done.

B) ONLINE FLOW â€” Answering a user query (RAG)

User asks: â€œWhat is FAISS used for?â€

You run:

python -m rag.test_rag


or later your API will do it.

Step B1 â€” Guardrails pre-check (optional stage)

detect prompt injection

detect unsafe requests

set policy: â€œcontext-only answeringâ€

Step B2 â€” Query embedding

embed the user query using same embedder

vector: shape (1, dim)

normalized

Step B3 â€” Retrieve top-k chunks from FAISS

index.search(q_vec, top_k)

returns:

indices: chunk ids in FAISS

scores: similarity scores

map indices â†’ metadata rows

produce retrieved chunks:

list of {doc_id, chunk_id, text}

Step B4 â€” Build the RAG prompt

You construct messages:

System message:

strict rules

output JSON schema

â€œanswer only from contextâ€

â€œrefuse if missingâ€

User message includes:

the retrieved CONTEXT chunks (with IDs)

the QUESTION

Step B5 â€” Call the LLM (generation)

send messages to OpenAI

model outputs text (should be JSON)

Step B6 â€” Parse and validate output schema

parse JSON into AnswerSchema

if invalid:

retry 1â€“3 times

if still invalid:

raise runtime error

Step B7 â€” Post-check (faithfulness / refusal)

(Optional but youâ€™ll implement)

verify citations refer to retrieved chunks

if not â†’ reject or force refusal

Step B8 â€” Return answer to user

answer text

confidence

citations

âœ… Online done.

Why this project is real engineering

Because you have:

reproducible offline pipeline

deterministic retrieval

explicit prompt contract

schema validation

refusal behavior

evaluation harness

Thatâ€™s what real LLM teams build.

Failure modes

Production realism

It reflects how real LLM applications are built â€” not demos, but reliable systems.
