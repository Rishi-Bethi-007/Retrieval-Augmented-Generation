[
  {
    "index_id": 0,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 0,
    "text": "An attention mechanism is a machine learning technique that directs deep learning models to prioritize (or attend to) the most relevant parts of input data. Innovation in attention mechanisms enabled the transformer architecture that yielded the modern large language models (LLMs) that power popular applications like ChatGPT."
  },
  {
    "index_id": 1,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 1,
    "text": "As their name suggests, attention mechanisms are inspired by the ability of humans (and other animals) to selectively pay more attention to salient details and ignore details that are less important in the moment."
  },
  {
    "index_id": 2,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 2,
    "text": "Having access to all information but focusing on only the most relevant information helps to ensure that no meaningful details are lost while enabling efficient use of limited memory and time. Mathematically speaking, an attention mechanism computes attention weights that reflect the relative importance of each part of an input sequence to the task at hand."
  },
  {
    "index_id": 3,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 3,
    "text": "It then applies those attention weights to increase (or decrease) the influence of each part of the input, in accordance with its respective importance. An attention model—that is, an artificial intelligence model that employs an attention mechanism—is trained to assign accurate attention weights through supervised learning or self-supervised learning on a large dataset of examples."
  },
  {
    "index_id": 4,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 4,
    "text": "Attention mechanisms were originally introduced by Bahdanau et al in 2014 as a technique to address the shortcomings of what were then state-of-the-art recurrent neural network (RNN) models used for machine translation. Subsequent research integrated attention mechanisms into the convolutional neural networks (CNNs) used for tasks such as image captioning and visual question answering."
  },
  {
    "index_id": 5,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 5,
    "text": "In 2017, the seminal paper “Attention is All You Need” introduced the transformer model, which eschews recurrence and convolutions altogether in favor of only attention layers and standard feedforward layers. The transformer architecture has since become the backbone of the cutting-edge models powering the ongoing era of generative AI."
  },
  {
    "index_id": 6,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 6,
    "text": "While attention mechanisms are primarily associated with LLMs used for natural language processing (NLP) tasks, such as summarization, question answering, text generation and sentiment analysis, attention-based models are also used widely in other domains. Leading diffusion models used for image generation often incorporate an attention mechanism."
  },
  {
    "index_id": 7,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 7,
    "text": "In the field of computer vision, vision transformers (ViTs) have achieved superior results on tasks including object detection,1 image segmentation2 and visual question answering.3\n\nIndustry newsletter\n\nThe latest AI trends, brought to you by experts\nGet curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement."
  },
  {
    "index_id": 8,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 8,
    "text": "Form not found\n\nWhy are attention mechanisms important? Transformer models and the attention mechanisms that power them have achieved state-of-the-art results across nearly every subdomain of deep learning."
  },
  {
    "index_id": 9,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 9,
    "text": "The nature of attention mechanisms gives them significant advantages over the convolution mechanisms used in convolutional neural networks (CNNs) and recurrent loops used in recurrent neural networks (RNNs). Flexibility over time: The way RNNs process sequential data is inherently serialized, meaning that they process each timestep in a sequence individually in a specific order."
  },
  {
    "index_id": 10,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 10,
    "text": "This makes it difficult for an RNN to discern correlations—called dependencies, in the parlance of data science—that have many steps in between them. Attention mechanisms, conversely, can examine an entire sequence simultaneously and make decisions about the order in which to focus on specific steps."
  },
  {
    "index_id": 11,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 11,
    "text": "Flexibility over space: CNNs are inherently local, using convolutions to process smaller subsets of input data one piece at a time. This makes it difficult for a CNN to discern dependencies that are far apart, such as correlations between words (in text) or pixels (in images) that aren’t neighboring one another."
  },
  {
    "index_id": 12,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 12,
    "text": "Attention mechanisms don’t have this limitation, as they process data in an entirely different way. Parallelization: The nature of attention mechanisms entails many computational steps being done at once, rather than in a serialized manner. This, in turns, enables a high degree of parallel computing, taking advantage of the power and speed offered by GPUs."
  },
  {
    "index_id": 13,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 13,
    "text": "To understand how attention mechanisms in deep learning work and why they helped spark a revolution in generative AI, it helps to first understand why attention was first introduced: to improve the RNN-based Seq2Seq models used for machine translation."
  },
  {
    "index_id": 14,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 14,
    "text": "How Seq2Seq works without attention mechanisms\nRNNs are neural networks with recurrent loops that provide an equivalent of “memory,” enabling them to process sequential data. RNNs intake an ordered sequence of input vectors and process them in timesteps. After each timestep, the resulting network state—called the hidden state—is provided back to the loop, along with the next input vector."
  },
  {
    "index_id": 15,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 15,
    "text": "RNNs quickly suffer from vanishing or exploding gradients in training. This made RNNs impractical for many NLP tasks, as it greatly limited the length of input sentences they could process.4 These limitations were somewhat mitigated by an improved RNN architecture called long short term memory networks (LSTMs), which add gating mechanisms to preserve “long term” memory."
  },
  {
    "index_id": 16,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 16,
    "text": "Before attention was introduced, the Seq2Seq model was the state-of-the-art model for machine translation. Seq2Seq uses two LSTMs in an encoder-decoder architecture. The first LSTM, the encoder, processes the source sentence step by step, then outputs the hidden state of the final timestep. This output, the context vector, encodes the whole sentence as one vector embedding."
  },
  {
    "index_id": 17,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 17,
    "text": "To enable Seq2Seq to flexibly handle sentences with varying numbers of word, the context vector is always the same length. The second LSTM, the decoder, takes the vector embedding output by the encoder as its initial input and decodes it, word by word, into a second language."
  },
  {
    "index_id": 18,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 18,
    "text": "Encoding input sequences in a fixed number of dimensions allowed Seq2Seq to process sequences of varying length, but also introduced important flaws:\n\nIt represents long or complex sequences with the same level of detail as shorter, simpler sentences. This causes an information bottleneck for longer sequences and wastes resources for shorter sequences."
  },
  {
    "index_id": 19,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 19,
    "text": "This vector represents only the final hidden state of the encoder network. In theory, each subsequent hidden state should contain information provided by the previous hidden state, which in turn contains information from the prior time step, and so on, back to the first step."
  },
  {
    "index_id": 20,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 20,
    "text": "In practice, the context vector inevitably “forgets” information from early time steps, hindering model performance on lengthier sequences."
  },
  {
    "index_id": 21,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 21,
    "text": "How attention mechanisms improved Seq2Seq\nBahdanau et al proposed an attention mechanism in their 2014 paper, “Neural Machine Translation by Jointly Learning to Align and Translate,” to improve communication between the encoder and decoder and remove that information bottleneck."
  },
  {
    "index_id": 22,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 22,
    "text": "Instead of passing along only the final hidden state of the encoder—the context vector—to the decoder, their model passed every encoder hidden state to the decoder. The attention mechanism itself was used to determine which hidden state—that is, which word in the original sentence—was most relevant at each translation step performed by the decoder."
  },
  {
    "index_id": 23,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 23,
    "text": "“This frees the model from having to encode a whole source sentence into a fixed-length vector, and also lets the model focus only on information relevant to the generation of the next target word,” the paper explained."
  },
  {
    "index_id": 24,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 24,
    "text": "“This has a major positive impact on the ability of the neural machine translation system to yield good results on longer sentences.\"5\n\nSubsequent NLP research focused primarily on improving performance and expanding use cases for attention mechanisms in recurrent models. The 2017 invention of transformer models, powered solely by attention, eventually made RNNs all but obsolete for NLP."
  },
  {
    "index_id": 25,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 25,
    "text": "Mixture of Experts | 23 January, episode 91\n\nIs Claude Code having a ChatGPT moment? Decoding AI: Weekly News Roundup\nJoin our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights. Watch all episodes of Mixture of Experts \nHow do attention mechanisms work?"
  },
  {
    "index_id": 26,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 26,
    "text": "An attention mechanism’s primary purpose is to determine the relative importance of different parts of the input sequence, then influence the model to attend to important parts and disregard unimportant parts."
  },
  {
    "index_id": 27,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 27,
    "text": "Though there are many variants and categories of attention mechanisms, each suited to different use cases and priorities, all attention mechanisms feature three core processes:\n\n A process of “reading” raw data sequences and converting them into vector embeddings, in which each element in the sequence is represented by its own feature vector(s)."
  },
  {
    "index_id": 28,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 28,
    "text": "A process of accurately determining similarities, correlations and other dependencies (or lack thereof) between each vector, quantified as alignment scores (or attention scores) that reflect how aligned (or not aligned) they are."
  },
  {
    "index_id": 29,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 29,
    "text": "Alignment scores are then used to compute attention weights by using a softmax function, which normalizes all values to a range between 0–1 such that they all add up to a total of 1. So for instance, assigning an attention weight of 0 to an element means it should be ignored."
  },
  {
    "index_id": 30,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 30,
    "text": "An attention weight of 1 means that element should receive 100% attention because all other elements would have attention weights of 0 (because all weights must sum up to 1). In essence, the output of a softmax function is a probability distribution. A process of using those attention weights to emphasize or deemphasize the influence of specific input elements on how the model makes predictions."
  },
  {
    "index_id": 31,
    "doc_id": "attention_mechanism.txt",
    "chunk_id": 31,
    "text": "In other words, a means of using attention weights to help models focus on or ignore information."
  },
  {
    "index_id": 32,
    "doc_id": "embeddings.txt",
    "chunk_id": 0,
    "text": "In machine learning, embeddings are a way of representing data as numerical vectors in a continuous space. They capture the meaning or relationship between data points, so that similar items are placed closer together while dissimilar ones are farther apart. This makes it easier for algorithms to work with complex data such as words, images or audios in a recommendation system."
  },
  {
    "index_id": 33,
    "doc_id": "embeddings.txt",
    "chunk_id": 1,
    "text": "They convert categorical or high-dimensional data into dense vectors. They help machine learning models work with different types of data. These vectors help show what the objects mean and how they relate to each other. They are widely used in natural language processing, recommender systems and computer vision."
  },
  {
    "index_id": 34,
    "doc_id": "embeddings.txt",
    "chunk_id": 2,
    "text": "Screenshot-2025-07-24-131534\nWord\nIn the above graph, we observe distinct clusters of related words. For instance \"computer\", \"software\" and \"machine\" are clustered together, indicating their semantic similarity. Similarly \"lion\", \"cow\" ,\"cat\" and \"dog\" form another cluster, representing their shared attributes."
  },
  {
    "index_id": 35,
    "doc_id": "embeddings.txt",
    "chunk_id": 3,
    "text": "There exists a significant gap between these clusters highlighting their dissimilarity in meaning or context. Key terms used for Embedding\n1. Vector\nA vector is a list of numbers that describes a size (magnitude) and a direction. In machine learning, it usually means a set of numbers that shows features or characteristics of something."
  },
  {
    "index_id": 36,
    "doc_id": "embeddings.txt",
    "chunk_id": 4,
    "text": "Example: In 2D, the vector points 3 steps along the x-axis and 4 steps along the y-axis. Its total length (magnitude) is 5. 2. Dense Vector\nA dense vector is a type of vector where most numbers are not zero. In machine learning, dense vectors are often used to describe things like words, images or data points because they capture a lot of details."
  },
  {
    "index_id": 37,
    "doc_id": "embeddings.txt",
    "chunk_id": 5,
    "text": "Example: [2000, 3, 5, 9.8] could describe a house, showing size, number of bedrooms, bathrooms and age. 3. Vector space\nA vector space or linear space is a mathematical structure consisting of a set of vectors that can be added together and multiplied by scalars, satisfying certain properties. It satisfy the certain properties like Closure under addition and Scalar multiplication."
  },
  {
    "index_id": 38,
    "doc_id": "embeddings.txt",
    "chunk_id": 6,
    "text": "Example: The set of all 3D vectors with real-number coordinates forms a vector space like the vectors [1, 0, 0], [0, 1, 0] and [0, 0, 1] constitute a basis for the 3D vector space. 4. Continuous Vector space\nA continuous vector space is a special kind of vector space where each value can be any real number (not just whole numbers)."
  },
  {
    "index_id": 39,
    "doc_id": "embeddings.txt",
    "chunk_id": 7,
    "text": "In embeddings, it means every object can be described with numbers that can smoothly change. Example: The color [0.9, 0.3, 0.1] in RGB shows a shade of red, where each number can be any value between 0 and 1. How do Embeddings Work? Let's see how embeddings work:\n\n1. Define similarity signal:\nFirst, decide what we want the model to treat as “similar”."
  },
  {
    "index_id": 40,
    "doc_id": "embeddings.txt",
    "chunk_id": 8,
    "text": "Text: Words or sentences that appear in similar contexts. Images: Pictures of the same object or scene. Graphs: Nodes that are connected or related. 2. Choose dimensionality:\nSelect how many numbers (dimensions) will describe each item, it could be 64, 384, 768 or more. More dimensions: more detail but slower and uses more memory. Fewer dimensions: faster but may lose detail. 3."
  },
  {
    "index_id": 41,
    "doc_id": "embeddings.txt",
    "chunk_id": 9,
    "text": "Build the encoder\nThis is the model that turns our data into a list of numbers (vector):\n\nText: Language models like BERT. Images: Vision models like CNN or ViT. Audio: Models that process sound (e.g., turning it into spectrograms first). Graphs: Methods like Node2Vec or graph neural networks. Tabular data: Models that compress features into embeddings. 4."
  },
  {
    "index_id": 42,
    "doc_id": "embeddings.txt",
    "chunk_id": 10,
    "text": "Train with a metric-learning objective:\nShow the model examples of things that are “similar” and “different.”\nTeach it to place similar ones close together and different ones far apart. This process is called metric learning. 5. Negative sampling and batching:\nGive the model tricky “hard negative” examples, things that seem alike but aren’t so it learns to tell them apart better. 6."
  },
  {
    "index_id": 43,
    "doc_id": "embeddings.txt",
    "chunk_id": 11,
    "text": "Validate and Tune\nTest how well our embeddings work by checking:\n\nHow accurate search results are. How well items group into the right categories. How good automatic clustering is. If the results aren’t good, adjust vector size, training method or data. 7."
  },
  {
    "index_id": 44,
    "doc_id": "embeddings.txt",
    "chunk_id": 12,
    "text": "Index for Fast Retrieval\nStore our vectors in a special database like Qdrant or FAISS to quickly find the closest matches, even from millions of items. 8. Use the embeddings\nOnce ready, embeddings can be used for:\n\nSemantic search: finding by meaning, not exact words. RAG (Retrieval-Augmented Generation): feeding relevant facts to an AI model."
  },
  {
    "index_id": 45,
    "doc_id": "embeddings.txt",
    "chunk_id": 13,
    "text": "Classification: predicting the correct label or category. Clustering: grouping similar items together. Recommendations: suggesting similar products, content or users. Monitoring: spotting unusual changes or patterns over time."
  },
  {
    "index_id": 46,
    "doc_id": "embeddings.txt",
    "chunk_id": 14,
    "text": "Importance of Embedding\nEmbeddings are used across various domains and tasks for several reasons:\n\nSemantic Representation: Embeddings capture semantic relationships between entities in the data. For example, in word embeddings, words with similar meanings are mapped to nearby points in the vector space."
  },
  {
    "index_id": 47,
    "doc_id": "embeddings.txt",
    "chunk_id": 15,
    "text": "Dimensionality Reduction: Embeddings reduce the dimensionality of data while preserving important features and relationships. Transfer Learning: Embeddings learned from one task or domain can be transferred and fine-tuned for use in related tasks or domains. Feature Engineering: Embeddings automatically extract meaningful features from raw data, reducing the need for manual feature engineering."
  },
  {
    "index_id": 48,
    "doc_id": "embeddings.txt",
    "chunk_id": 16,
    "text": "Interpretability: Embeddings provide interpretable representations of data. For example, in word embeddings, the direction and distance between word vectors can correspond to meaningful relationships such as gender, tense or sentiment."
  },
  {
    "index_id": 49,
    "doc_id": "embeddings.txt",
    "chunk_id": 17,
    "text": "Objects that can be Embedded\nFrom textual data to images and beyond, embeddings offer a versatile approach to encoding information into dense vector representations. Some of the major types of objects or values that can be embedded include:\n\n1. Words\nWord embeddings are numeric vectors that represent words in a continuous space, where similar words are placed near each other."
  },
  {
    "index_id": 50,
    "doc_id": "embeddings.txt",
    "chunk_id": 18,
    "text": "These vectors are learned from large text datasets and capture the meanings and relationships between words making it easier for computers to understand and process language in tasks like sentiment analysis and translation."
  },
  {
    "index_id": 51,
    "doc_id": "embeddings.txt",
    "chunk_id": 19,
    "text": "Some of the Popular word embeddings include:\n\nWord2Vec\nGloVe (Global Vectors for Word Representation)\nFastText\nBERT (Bidirectional Encoder Representations from Transformers)\nGPT\n2. Complete Text Document\nText embeddings or document embeddings represent entire sentences, paragraphs or documents as numeric vectors in a continuous space."
  },
  {
    "index_id": 52,
    "doc_id": "embeddings.txt",
    "chunk_id": 20,
    "text": "Unlike word embeddings that focus on single words, text embeddings capture the meaning and context of longer text segments. This allows for easier comparison and analysis of complete pieces of text in NLP tasks like sentiment analysis, translation or document classification. Some of the Popular text embedding models include:\n\nDoc2Vec\nUniversal Sentence Encoder (USE)\nBERT\nELMO\n3."
  },
  {
    "index_id": 53,
    "doc_id": "embeddings.txt",
    "chunk_id": 21,
    "text": "Audio Data\nAudio data includes individual sound samples, audio clips and entire audio recordings. By representing audio as dense vectors in a continuous vector space, embedding techniques effectively capture acoustic features and relationships. This enables a wide range of audio processing tasks such as speech recognition, speaker identification, emotion detection and music genre classification."
  },
  {
    "index_id": 54,
    "doc_id": "embeddings.txt",
    "chunk_id": 22,
    "text": "Some of the popular Audio embedding techniques may include Wav2Vec\n\n4. Image Data\nImage embeddings are numerical representations of images in a continuous vector space, extracted by processing images through convolutional neural networks (CNNs)."
  },
  {
    "index_id": 55,
    "doc_id": "embeddings.txt",
    "chunk_id": 23,
    "text": "These embeddings encode the visual content, features and semantics of images, facilitating efficient understanding and processing of visual information by machines. Some of the popular CNNs based Image embedding techniques include:\n\nVGG\nResNet\nInception\nEfficientNet\n5."
  },
  {
    "index_id": 56,
    "doc_id": "embeddings.txt",
    "chunk_id": 24,
    "text": "Graph Data\nGraph embeddings convert a graph’s nodes and edges into numeric vectors, capturing the graph’s structure and relationships. This representation makes complex graph data easier for machine learning models to use enabling tasks like node classification, link prediction and clustering. Some popular graph embedding techniques include:\n\nNode2Vec\nDeepWalk\nGraph Convolutional Networks\n6."
  },
  {
    "index_id": 57,
    "doc_id": "embeddings.txt",
    "chunk_id": 25,
    "text": "Structured Data\nStructured data such as feature vectors and tables can be embedded to help machine learning models capture underlying patterns. Common techniques include Autoencoders\n\n\n\nAI embeddings offer the potential to generate superior training data, enhancing data quality and minimizing manual labeling requirements."
  },
  {
    "index_id": 58,
    "doc_id": "embeddings.txt",
    "chunk_id": 26,
    "text": "By converting input data into machine-readable formats, businesses can leverage AI technology to transform workflows, streamline processes, and optimize performance. Machine learning is a powerful tool that has the potential to transform the way we live and work. However, the success of any machine learning model depends heavily on the quality of the training data that is used to develop it."
  },
  {
    "index_id": 59,
    "doc_id": "embeddings.txt",
    "chunk_id": 27,
    "text": "High-quality training data is often considered to be the most critical factor in achieving accurate and reliable machine learning results. In this blog, we’ll discuss the importance of high-quality training data in machine learning and how AI embeddings can help improve it."
  },
  {
    "index_id": 60,
    "doc_id": "embeddings.txt",
    "chunk_id": 28,
    "text": "We will cover:\n\nImportance of high-quality training data\nCreating high-quality training data using AI embeddings\nCase studies demonstrating the use of embeddings \nBest practices for using AI embeddings\nLeverage embeddings to analyze and fix your dataset and labels using Encord Active\nLearn more\nmedical banner\nImportance of High-Quality Training Data\nThe importance of high-quality training data in"
  },
  {
    "index_id": 61,
    "doc_id": "embeddings.txt",
    "chunk_id": 29,
    "text": "machine learning lies in the fact that it directly impacts the accuracy and reliability of machine learning models."
  },
  {
    "index_id": 62,
    "doc_id": "embeddings.txt",
    "chunk_id": 30,
    "text": "For a model to accurately learn patterns and make predictions, it needs to be trained on large volumes of diverse, accurate, and unbiased data. If the data used for training is low-quality or contains inaccuracies and biases, it will produce less accurate and potentially biased predictions."
  },
  {
    "index_id": 63,
    "doc_id": "embeddings.txt",
    "chunk_id": 31,
    "text": "The quality of datasets being used to train models applies to every type of AI model, including Foundation Models, such as ChatGPT and Google’s BERT. The Washington Post took a closer look at the vast datasets being used to train some of the world’s most popular and powerful large language models (LLMs)."
  },
  {
    "index_id": 64,
    "doc_id": "embeddings.txt",
    "chunk_id": 32,
    "text": "In particular, the article reviewed the content of Google’s C4 dataset, finding that quality and quantity are equally important, especially when training LLMs. In image recognition tasks, if the training data used to teach the model contains images with inaccurate or incomplete labels, then the model may not be able to recognize or classify similar images in its predictions accurately."
  },
  {
    "index_id": 65,
    "doc_id": "embeddings.txt",
    "chunk_id": 33,
    "text": "At the same time, if the training data is biased towards certain groups or demographics, then the model may learn and replicate those biases, leading to unfair or discriminatory treatment of certain groups. For instance, Google, too, succumbed to bias traps in a recent incident where its Vision AI model generated racist outcomes."
  },
  {
    "index_id": 66,
    "doc_id": "embeddings.txt",
    "chunk_id": 34,
    "text": "blog_image_4765\n\nThe images in the BDD dataset have a pedestrian labeled as remote and book, which is clearly annotated wrongly. Hence, using high-quality training data is crucial to ensuring accurate and unbiased machine learning models."
  },
  {
    "index_id": 67,
    "doc_id": "embeddings.txt",
    "chunk_id": 35,
    "text": "This involves selecting appropriate and diverse data sources and ensuring the data is cleaned, preprocessed, and labeled accurately before being used for training. What is an Embedding in Machine Learning? In artificial intelligence, an embedding is a mathematical representation of a set of data points in a lower-dimensional space that captures their underlying relationships and patterns."
  },
  {
    "index_id": 68,
    "doc_id": "embeddings.txt",
    "chunk_id": 36,
    "text": "Embeddings are"
  },
  {
    "index_id": 69,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 0,
    "text": "FAISS (Facebook AI Similarity Search) is an open-source library developed by Meta (formerly Facebook) for efficient similarity search and clustering of dense vectors."
  },
  {
    "index_id": 70,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 1,
    "text": "It is designed to handle massive datasets—ranging from millions to billions of high-dimensional vectors—making it ideal for applications like recommendation systems, image retrieval, natural language processing, and content moderation."
  },
  {
    "index_id": 71,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 2,
    "text": "Key Features\nHigh Performance: Optimized for speed and memory efficiency using advanced algorithms like Product Quantization (PQ), Inverted-File Indexes (IVF), HNSW, and NSG graphs. GPU Acceleration: Many algorithms are implemented on GPU using CUDA, enabling fast approximate nearest neighbor (ANN) search at scale."
  },
  {
    "index_id": 72,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 3,
    "text": "Flexible Indexing: Supports multiple indexing methods including brute-force (IndexFlatL2), compressed indices, and graph-based approaches. Vector Compression: Includes lossy compression via vector quantization, allowing trade-offs between accuracy and storage size. Python & C++ Interfaces: Full Python wrappers (NumPy compatible) with seamless CPU/GPU support."
  },
  {
    "index_id": 73,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 4,
    "text": "Core Assumptions: Uses FP32 as the primary data type; supports BF16 and FP16; prefers batch queries over single queries. Use Cases\nRecommender Systems: Find similar items based on embedding vectors. Image & Video Search: Identify visually similar media using deep learning embeddings. Text Retrieval: Semantic search over document or sentence embeddings."
  },
  {
    "index_id": 74,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 5,
    "text": "Data Deduplication: Detect duplicate or near-duplicate data (e.g., images). Vector Databases: Serves as a foundational component in systems like Milvus, OpenSearch, and Vearch. Technical Highlights\nPrimary Distance Metrics: Euclidean (L2) and inner product (dot product); limited support for Manhattan and Lp distances."
  },
  {
    "index_id": 75,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 6,
    "text": "Built-in Tools: Includes k-means clustering, PCA, random rotation, and data deduplication. Stable Release: v1.12.0 (August 12, 2025). License: MIT License."
  },
  {
    "index_id": 76,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 7,
    "text": "Repository: github.com/facebookresearch/faiss\nFAISS is widely regarded as a benchmark standard in similarity search and is often used as a baseline in large-scale ANN competitions like NeurIPS and Big ANN\n\nvectors and enable fast similarity queries. The choice of index determines the trade-offs between speed, memory usage, and accuracy. Core Index Types\n1."
  },
  {
    "index_id": 77,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 8,
    "text": "Flat Index (Exact Search)\nDescription: Brute-force search that computes distances to all vectors. Accuracy: 100% (exact results). Use Case: Small datasets (<100K vectors). Variants:\nIndexFlatL2: Uses Euclidean (L2) distance. IndexFlatIP: Uses inner product (cosine similarity on normalized vectors). FAISS\nlibrary for similarity search\nWikipedia\nfaiss.ai\n2."
  },
  {
    "index_id": 78,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 9,
    "text": "Inverted File Index (IVF)\nMechanism: Partitions the vector space into nlist clusters using k-means. Search Process:\nFind the nprobe closest clusters to the query. Search only within those clusters. Speed vs Accuracy: Controlled by nprobe (higher = slower but more accurate). Base Index: Typically IndexIVFFlat, but can be combined with compression. FAISS IVF index explained\n\n\n\n\n\n3."
  },
  {
    "index_id": 79,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 10,
    "text": "Hierarchical Navigable Small World (HNSW)\nStructure: Multi-layered graph where nodes represent vectors. Search: Starts at the top layer (coarse view), navigates down to finer layers. Advantages: Very fast and accurate; no training required. Memory: Higher than IVF but excellent performance. Variant: IndexHNSWFlat (supports compression via IndexHNSWPQ). HNSW algorithm explained\n\n\n\n\n\n4."
  },
  {
    "index_id": 80,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 11,
    "text": "Product Quantization (PQ)\nConcept: Compresses vectors by splitting them into subvectors and quantizing each. Storage: Each subvector represented by a codebook index (e.g., 8 bits instead of 32-bit float). Search: Distance computed in compressed domain (faster and memory-efficient). Trade-off: Slight accuracy loss for significant speed and size gains. Product Quantization in FAISS\n\n5."
  },
  {
    "index_id": 81,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 12,
    "text": "IVF + PQ (IndexIVFPQ)\nHybrid Approach:\nIVF partitions data into clusters. PQ compresses vectors within each cluster. Efficiency: Only decompress and search in nprobe clusters. Ideal For: Large-scale datasets (millions to billions of vectors)."
  },
  {
    "index_id": 82,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 13,
    "text": "Example: index = faiss.IndexIVFPQ(quantizer, d=128, nlist=100, M=16, nbits=8)\n\nFAISS IVFPQ tuning parameters site:reddit.com\n\n\n\n\n\nDistance Metrics\nL2 (Euclidean): Default for IndexFlatL2, IVF, etc. Inner Product (IP): Used for cosine similarity (vectors should be normalized). Limited Support: L1 (Manhattan), Linf (Chebyshev). Binary Metrics: Hamming distance via IndexLSH."
  },
  {
    "index_id": 83,
    "doc_id": "faiss_overview.txt",
    "chunk_id": 14,
    "text": "GPU Acceleration\nFAISS supports CUDA for significant speedups. GPU versions mirror CPU indexes (e.g., GpuIndexFlatL2, GpuIndexIVFPQ). Multi-GPU and CPU-GPU interoperability supported."
  },
  {
    "index_id": 84,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 0,
    "text": "This conceptual guide gives a brief overview of LoRA, a technique that accelerates the fine-tuning of large models while consuming less memory. To make fine-tuning more efficient, LoRA’s approach is to represent the weight updates with two smaller matrices (called update matrices) through low-rank decomposition."
  },
  {
    "index_id": 85,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 1,
    "text": "These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn’t receive any further adjustments. To produce the final results, both the original and the adapted weights are combined."
  },
  {
    "index_id": 86,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 2,
    "text": "This approach has a number of advantages:\n\nLoRA makes fine-tuning more efficient by drastically reducing the number of trainable parameters. The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable LoRA models for various downstream tasks built on top of them."
  },
  {
    "index_id": 87,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 3,
    "text": "LoRA is orthogonal to many other parameter-efficient methods and can be combined with many of them. Performance of models fine-tuned using LoRA is comparable to the performance of fully fine-tuned models. LoRA does not add any inference latency because adapter weights can be merged with the base model."
  },
  {
    "index_id": 88,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 4,
    "text": "In principle, LoRA can be applied to any subset of weight matrices in a neural network to reduce the number of trainable parameters. However, for simplicity and further parameter efficiency, in Transformer models LoRA is typically applied to attention blocks only."
  },
  {
    "index_id": 89,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 5,
    "text": "The resulting number of trainable parameters in a LoRA model depends on the size of the low-rank update matrices, which is determined mainly by the rank r and the shape of the original weight matrix."
  },
  {
    "index_id": 90,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 6,
    "text": "Merge LoRA weights into the base model\nWhile LoRA is significantly smaller and faster to train, you may encounter latency issues during inference due to separately loading the base model and the LoRA model. To eliminate latency, use the merge_and_unload() function to merge the adapter weights with the base model which allows you to effectively use the newly merged model as a standalone model."
  },
  {
    "index_id": 91,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 7,
    "text": "This works because during training, the smaller weight matrices (A and B in the diagram above) are separate. But once training is complete, the weights can actually be merged into a new weight matrix that is identical. Utils for LoRA\nUse merge_adapter() to merge the LoRa layers into the base model while retaining the PeftModel."
  },
  {
    "index_id": 92,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 8,
    "text": "This will help in later unmerging, deleting, loading different adapters and so on. Use unmerge_adapter() to unmerge the LoRa layers from the base model while retaining the PeftModel. This will help in later merging, deleting, loading different adapters and so on. Use unload() to get back the base model without the merging of the active lora modules."
  },
  {
    "index_id": 93,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 9,
    "text": "This will help when you want to get back the pretrained base model in some applications when you want to reset the model to its original state. For example, in Stable Diffusion WebUi, when the user wants to infer with base model post trying out LoRAs. Use delete_adapter() to delete an existing adapter."
  },
  {
    "index_id": 94,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 10,
    "text": "Use add_weighted_adapter() to combine multiple LoRAs into a new adapter based on the user provided weighing scheme. Common LoRA parameters in PEFT\nAs with other methods supported by PEFT, to fine-tune a model using LoRA, you need to:\n\nInstantiate a base model. Create a configuration (LoraConfig) where you define LoRA-specific parameters."
  },
  {
    "index_id": 95,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 11,
    "text": "Wrap the base model with get_peft_model() to get a trainable PeftModel. Train the PeftModel as you normally would train the base model. LoraConfig allows you to control how LoRA is applied to the base model through the following parameters:\n\nr: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters."
  },
  {
    "index_id": 96,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 12,
    "text": "target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices. lora_alpha: LoRA scaling factor. bias: Specifies if the bias parameters should be trained. Can be 'none', 'all' or 'lora_only'. use_rslora: When set to True, uses Rank-Stabilized LoRA which sets the adapter scaling factor to lora_alpha/math.sqrt(r), since it was proven to work better."
  },
  {
    "index_id": 97,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 13,
    "text": "Otherwise, it will use the original default value of lora_alpha/r. modules_to_save: List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint. These typically include model’s custom head that is randomly initialized for the fine-tuning task. layers_to_transform: List of layers to be transformed by LoRA."
  },
  {
    "index_id": 98,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 14,
    "text": "If not specified, all layers in target_modules are transformed. layers_pattern: Pattern to match layer names in target_modules, if layers_to_transform is specified. By default PeftModel will look at common layer pattern (layers, h, blocks, etc.), use it for exotic and custom models."
  },
  {
    "index_id": 99,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 15,
    "text": "rank_pattern: The mapping from layer names or regexp expression to ranks which are different from the default rank specified by r. alpha_pattern: The mapping from layer names or regexp expression to alphas which are different from the default alpha specified by lora_alpha."
  },
  {
    "index_id": 100,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 16,
    "text": "LoRA examples\nFor an example of LoRA method application to various downstream tasks, please refer to the following guides:\n\nImage classification using LoRA\nSemantic segmentation\nWhile the original paper focuses on language models, the technique can be applied to any dense layers in deep learning models. As such, you can leverage this technique with diffusion models."
  },
  {
    "index_id": 101,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 17,
    "text": "See Dreambooth fine-tuning with LoRA task guide for an example. Initialization options\nThe initialization of LoRA weights is controlled by the parameter init_lora_weights of the LoraConfig. By default, PEFT initializes LoRA weights the same way as the reference implementation, i.e. using Kaiming-uniform for weight A and initializing weight B as zeros, resulting in an identity transform."
  },
  {
    "index_id": 102,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 18,
    "text": "It is also possible to pass init_lora_weights=\"gaussian\". As the name suggests, this results in initializing weight A with a Gaussian distribution (weight B is still zeros). This corresponds to the way that diffusers initializes LoRA weights. When quantizing the base model, e.g."
  },
  {
    "index_id": 103,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 19,
    "text": "for QLoRA training, consider using the LoftQ initialization, which has been shown to improve the performance with quantization. The idea is that the LoRA weights are initialized such that the quantization error is minimized. To use this option, do not quantize the base model."
  },
  {
    "index_id": 104,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 20,
    "text": "Instead, proceed as follows:\n\nCopied\nfrom peft import LoftQConfig, LoraConfig, get_peft_model\n\nbase_model = AutoModelForCausalLM.from_pretrained(...)  # don't quantize here\nloftq_config = LoftQConfig(loftq_bits=4, ...)           # set 4bit quantization\nlora_config = LoraConfig(..., init_lora_weights=\"loftq\", loftq_config=loftq_config)\npeft_model = get_peft_model(base_model, lora_config)\nThere is a"
  },
  {
    "index_id": 105,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 21,
    "text": "lso an option to set initialize_lora_weights=False."
  },
  {
    "index_id": 106,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 22,
    "text": "When choosing this option, the LoRA weights are initialized such that they do not result in an identity transform. This is useful for debugging and testing purposes and should not be used otherwise. Finally, the LoRA architecture scales each adapter during every forward pass by a fixed scalar, which is set at initialization, and depends on the rank r."
  },
  {
    "index_id": 107,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 23,
    "text": "Although the original LoRA method uses the scalar function lora_alpha/r, the research Rank-Stabilized LoRA proves that instead using lora_alpha/math.sqrt(r), stabilizes the adapters and unlocks the increased performance potential from higher ranks. Set use_rslora=True to use the rank-stabilized scaling lora_alpha/math.sqrt(r)."
  },
  {
    "index_id": 108,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 24,
    "text": "Low-rank adaptation (LoRA) fine tuning adapts a foundation model for a task by changing the weights of a representative subset of the model parameters, called low-rank adapters, instead of the base model weights during tuning. At inference time, weights from the tuned adapters are added to the weights from the base foundation model to generate output that is tuned for a task."
  },
  {
    "index_id": 109,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 25,
    "text": "Note: Starting with the 2.1.1 release, you can run a LoRA or QLoRA tuning experiment programmatically, not from the Tuning Studio."
  },
  {
    "index_id": 110,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 26,
    "text": "How low-rank adaptation (LoRA) tuning works\nLow-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) technique that adds a subset of parameters to the frozen base foundation model and updates the subset during the tuning experiment, without modifying the parameters of the base model."
  },
  {
    "index_id": 111,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 27,
    "text": "When the tuned foundation model is inferenced, the new parameter weights from the subset are added to the parameter weights from the base model to generate output that is customized for a task. How the subset of parameters is created involves some mathematics. Remember, the neural network of a foundation model is composed of layers, each with a complex matrix of parameters."
  },
  {
    "index_id": 112,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 28,
    "text": "These parameters have weight values that are set when the foundation model is initially trained. The subset of parameters that are used for LoRA tuning is derived by applying rank decomposition to the weights of the base foundation model. The rank of a matrix indicates the number of vectors in the matrix that are linearly independent from one another."
  },
  {
    "index_id": 113,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 29,
    "text": "Rank decomposition, also known as matrix decomposition, is a mathematical method that uses this rank information to represent the original matrix in two smaller matrices that, when multiplied, form a matrix that is the same size as the original matrix. With this method, the two smaller matrices together capture key patterns and relationships from the larger matrix, but with fewer parameters."
  },
  {
    "index_id": 114,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 30,
    "text": "The smaller matrices produced are called low-rank matrices or low-rank adapters. During a LoRA tuning experiment, the weight values of the parameters in the subset–the low-rank adapters–are adjusted. Because the adapters have fewer parameters, the tuning experiment is faster and needs fewer resources to store and compute changes."
  },
  {
    "index_id": 115,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 31,
    "text": "Although the adapter matrices lack some of the information from the base model matrices, the LoRA tuning method is effective because LoRA exploits the fact that large foundation models typically use many more parameters than are necessary for a task. The output of a LoRA fine-tuning experiment is a set of adapters that contain new weights."
  },
  {
    "index_id": 116,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 32,
    "text": "When these tuned adapters are multiplied, they form a matrix that is the same size as the matrix of the base model. At inference time, the new weights from the product of the adapters are added directly to the base model weights to generate the fine-tuned output."
  },
  {
    "index_id": 117,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 33,
    "text": "You can configure parameters of the LoRA tuning experiment, such as the base foundation model layers to target and the rank to use when decomposing the base model matrices. For more details, see Parameters for tuning foundation models. When you deploy the adapter asset, you must deploy the asset in a deployment space where the base model is also deployed."
  },
  {
    "index_id": 118,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 34,
    "text": "You can use the LoRA fine tuning method in watsonx.ai to fine tune only non-quantized foundation models. The benefits of using the LoRA fine-tuning technique include:\n\nThe smaller, trainable adapters used by the LoRA technique require fewer storage and computational resources during tuning."
  },
  {
    "index_id": 119,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 35,
    "text": "Adjustments from the adapters are applied at inference time without impacting the context window length or the speed of model responses. You can deploy one base foundation model and use the model with different adapters to customize outputs for different tasks."
  },
  {
    "index_id": 120,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 36,
    "text": "Low-rank adaptation (LoRA) fine-tuning workflow\nDuring the LoRA fine-tuning experiment, the parameter weights of a representative subset of the model parameters, called low-rank adapters, are repeatedly adjusted so that the predictions of the tuned foundation model can get better over time. The following diagram illustrates the steps that occur during a LoRA fine-tuning experiment run."
  },
  {
    "index_id": 121,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 37,
    "text": "The parts of the experiment flow that you can configure are highlighted with a user icon user. These decision points correspond with experiment tuning parameters that you control. See Parameters for tuning foundation models."
  },
  {
    "index_id": 122,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 38,
    "text": "Low-rank adaptation fine tuning process details that are described in the steps\n\nThe diagram shows the following steps of the experiment:\n\nThe experiment reads the training data, tokenizes it, and converts it into batches. The size of the batches is determined by the batch size parameter. Low-rank adapters, which are a representative subset of the base model parameters, are devised."
  },
  {
    "index_id": 123,
    "doc_id": "lora_finetuning.txt",
    "chunk_id": 39,
    "text": "The initial weights of the low-rank adapters are applied to the model layers that you specify in the target_modules parameter, and are calculated based on the value that you specify for the rank parameter."
  },
  {
    "index_id": 124,
    "doc_id": "transformers.txt",
    "chunk_id": 0,
    "text": "Transformer is a neural network architecture used for performing machine learning tasks particularly in natural language processing (NLP) and computer vision. In 2017 Vaswani et al. published a paper \" Attention is All You Need\" in which the transformers architecture was introduced. The article explores the architecture, workings and applications of transformers."
  },
  {
    "index_id": 125,
    "doc_id": "transformers.txt",
    "chunk_id": 1,
    "text": "_what_are_transformers_.webp_what_are_transformers_.webp\nNeed For Transformers Model in Machine Learning\nTransformer architecture uses an attention mechanism to process an entire sentence at once instead of reading words one by one. This is useful because older models work step by step and it helps overcome the challenges seen in models like RNNs and LSTMs."
  },
  {
    "index_id": 126,
    "doc_id": "transformers.txt",
    "chunk_id": 2,
    "text": "Traditional models like RNNs (Recurrent Neural Networks) suffer from the vanishing gradient problem which leads to long-term memory loss. RNNs process text sequentially meaning they analyze words one at a time. For example:\n\n In the sentence: \"XYZ went to France in 2019 when there were no cases of COVID and there he met the president of that country\" the word \"that country\" refers to \"France\"."
  },
  {
    "index_id": 127,
    "doc_id": "transformers.txt",
    "chunk_id": 3,
    "text": "However RNN would struggle to link \"that country\" to \"France\" since it processes each word in sequence leading to losing context over long sentences. This limitation prevents RNNs from understanding the full meaning of the sentence. While adding more memory cells in LSTMs (Long Short-Term Memory networks) helped address the vanishing gradient issue they still process words one by one."
  },
  {
    "index_id": 128,
    "doc_id": "transformers.txt",
    "chunk_id": 4,
    "text": "This sequential processing means LSTMs can't analyze an entire sentence at once."
  },
  {
    "index_id": 129,
    "doc_id": "transformers.txt",
    "chunk_id": 5,
    "text": "For example:\n\n The word \"point\" has different meanings in these two sentences:\n\n\"The needle has a sharp point.\" (Point = Tip)\n\"It is not polite to point at people.\" (Point = Gesture)\nTraditional models struggle with this context dependence, whereas Transformer model through its self-attention mechanism processes the entire sentence in parallel addressing these issues and making it significantly mo"
  },
  {
    "index_id": 130,
    "doc_id": "transformers.txt",
    "chunk_id": 6,
    "text": "re effective at understanding context."
  },
  {
    "index_id": 131,
    "doc_id": "transformers.txt",
    "chunk_id": 7,
    "text": "Core Concepts of Transformers\ntransformers\nArchitecture of Transformer\n1. Self Attention Mechanism\nThe self attention mechanism allows transformers to determine which words in a sentence are most relevant to each other."
  },
  {
    "index_id": 132,
    "doc_id": "transformers.txt",
    "chunk_id": 8,
    "text": "This is done using a scaled dot-product attention approach:\n\nEach word in a sequence is mapped to three vectors:\n\nThese scores determine how much attention each word should pay to others. 2. Multi-Head Attention\nInstead of one attention mechanism, transformers use multiple attention heads running in parallel."
  },
  {
    "index_id": 133,
    "doc_id": "transformers.txt",
    "chunk_id": 9,
    "text": "Each head captures different relationships or patterns in the data, enriching the model’s understanding. 3. Positional Encoding\nUnlike RNNs, transformers lack an inherent understanding of word order since they process data in parallel. To solve this problem Positional Encodings are added to token embeddings providing information about the position of each token within a sequence. 4."
  },
  {
    "index_id": 134,
    "doc_id": "transformers.txt",
    "chunk_id": 10,
    "text": "Position-wise Feed-Forward Networks\nThe Feed-Forward Networks consist of two linear transformations with a ReLU activation. It is applied independently to each position in the sequence. Mathematically:\n​\n \n\nThis transformation helps refine the encoded representation at each position. 5. Embeddings\nTransformers cannot work with raw words as they need numbers."
  },
  {
    "index_id": 135,
    "doc_id": "transformers.txt",
    "chunk_id": 11,
    "text": "So, each input token (word or subword) is converted into a vector, called an embedding. Both encoder input tokens and decoder input tokens are converted into embeddings. These embeddings are trainable, meaning the model learns the best numeric representation for each token."
  },
  {
    "index_id": 136,
    "doc_id": "transformers.txt",
    "chunk_id": 12,
    "text": "The same weight matrix is shared for Encoder embeddings, Decoder embeddings and the final linear layer before softmax\nThe embeddings are scaled by model to keep values stable before adding positional encoding. Embeddings turn words into meaningful numeric vectors that the transformer can process. 6. Encoder-Decoder Architecture\nThe encoder-decoder structure is key to transformer models."
  },
  {
    "index_id": 137,
    "doc_id": "transformers.txt",
    "chunk_id": 13,
    "text": "The encoder processes the input sequence into a vector, while the decoder converts this vector back into a sequence. Each encoder and decoder layer includes self-attention and feed-forward layers. For example, a French sentence \"Je suis étudiant\" is translated into \"I am a student\" in English. Transformers apply attention in three different places:\n\n1."
  },
  {
    "index_id": 138,
    "doc_id": "transformers.txt",
    "chunk_id": 14,
    "text": "Encoder Self-Attention\n\nQ, K, V all come from the encoder’s previous layer. Every word can attend to every other word in the input. This helps the encoder understand full context (long-range meaning). 2. Decoder Self-Attention (Masked)\n\nQ, K, V all come from the decoder’s previous layer. Future tokens are masked (blocked), so each position only sees previous tokens."
  },
  {
    "index_id": 139,
    "doc_id": "transformers.txt",
    "chunk_id": 15,
    "text": "This keeps decoding auto-regressive i.e the model predicts one word at a time. 3. Encoder–Decoder Attention\n\nQueries come from the decoder. Keys and Values come from the encoder output. This lets the decoder look at important parts of the input sentence while generating output."
  },
  {
    "index_id": 140,
    "doc_id": "transformers.txt",
    "chunk_id": 16,
    "text": "Together, these three attention types allow the transformer to read the entire input at once and then generate outputs step-by-step with full context. 7. Softmax Layer for Output Prediction\nAfter the decoder processes the sequence, it must predict the next token. The decoder output is passed through a linear layer (whose weights are shared with embeddings)."
  },
  {
    "index_id": 141,
    "doc_id": "transformers.txt",
    "chunk_id": 17,
    "text": "Then the softmax function converts these scores into probabilities. The token with the highest probability becomes the predicted next word. Intuition with Example\nFor instance in the sentence \"The cat didn't chase the mouse, because it was not hungry\" the word 'it' refers to 'cat'."
  },
  {
    "index_id": 142,
    "doc_id": "transformers.txt",
    "chunk_id": 18,
    "text": "The self-attention mechanism helps the model correctly associate 'it' with 'cat' ensuring an accurate understanding of sentence structure. Applications\nSome of the applications of transformers are:\n\nNLP Tasks: Transformers are used for machine translation, text summarization, named entity recognition and sentiment analysis."
  },
  {
    "index_id": 143,
    "doc_id": "transformers.txt",
    "chunk_id": 19,
    "text": "Speech Recognition: They process audio signals to convert speech into transcribed text. Computer Vision: Transformers are applied to image classification, object detection and image generation. Recommendation Systems: They provide personalized recommendations based on user preferences. Text and Music Generation: Transformers are used for generating text like articles and composing music."
  },
  {
    "index_id": 144,
    "doc_id": "transformers.txt",
    "chunk_id": 20,
    "text": "Transformer (deep learning)\n\nArticle\nTalk\nRead\nEdit\nView history\n\nTools\nFrom Wikipedia, the free encyclopedia\n\nA standard transformer architecture, showing on the left an encoder, and on the right a decoder. Note: it uses the pre-LN convention, which is different from the post-LN convention used in the original 2017 transformer."
  },
  {
    "index_id": 145,
    "doc_id": "transformers.txt",
    "chunk_id": 21,
    "text": "Part of a series on\nMachine learning\nand data mining\nParadigms\nProblems\nSupervised learning\n(classification • regression)\nClustering\nDimensionality reduction\nStructured prediction\nAnomaly detection\nNeural networks\nAutoencoderDeep learningFeedforward neural networkRecurrent neural network LSTMGRUESNreservoir computingBoltzmann machine RestrictedGANDiffusion modelSOMConvolutional neural network U-Ne"
  },
  {
    "index_id": 146,
    "doc_id": "transformers.txt",
    "chunk_id": 22,
    "text": "tLeNetAlexNetDeepDreamNeural field Neural radiance fieldPhysics-informed neural networksTransformer VisionMambaSpiking neural networkMemtransistorElectrochemical RAM (ECRAM)\nReinforcement learning\nLearning with humans\nModel diagnostics\nMathematical foundations\nJournals and conferences\nRelated articles\nvte\nIn deep learning, the transformer is an artificial neural network architecture based on the m"
  },
  {
    "index_id": 147,
    "doc_id": "transformers.txt",
    "chunk_id": 23,
    "text": "ulti-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.[1] At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplifie"
  },
  {
    "index_id": 148,
    "doc_id": "transformers.txt",
    "chunk_id": 24,
    "text": "d and less important tokens to be diminished."
  },
  {
    "index_id": 149,
    "doc_id": "transformers.txt",
    "chunk_id": 25,
    "text": "Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).[2] Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.[3]\n\nThe modern version of the transformer was proposed in the 2017 paper \"Attention Is All Y"
  },
  {
    "index_id": 150,
    "doc_id": "transformers.txt",
    "chunk_id": 26,
    "text": "ou Need\" by researchers at Google.[1] The predecessors of transformers were developed as an improvement over previous architectures for machine translation,[4][5] but have found many applications since."
  },
  {
    "index_id": 151,
    "doc_id": "transformers.txt",
    "chunk_id": 27,
    "text": "They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,[6][7] audio,[8] multimodal learning, robotics,[9] and even playing chess.[10] It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs)[11] and BERT[12] (bidirectional encoder representations from transformers)."
  },
  {
    "index_id": 152,
    "doc_id": "transformers.txt",
    "chunk_id": 28,
    "text": "History\nSee also: Timeline of machine learning\nPredecessors\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990)."
  },
  {
    "index_id": 153,
    "doc_id": "transformers.txt",
    "chunk_id": 29,
    "text": "In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens."
  },
  {
    "index_id": 154,
    "doc_id": "transformers.txt",
    "chunk_id": 30,
    "text": "A key breakthrough was LSTM (1995),[note 1] an RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling."
  },
  {
    "index_id": 155,
    "doc_id": "transformers.txt",
    "chunk_id": 31,
    "text": "One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units.[13] Neural networks using multiplicative units were later called sigma-pi networks[14] or higher-order networks.[15] LSTM became the standard architecture for long sequence modelling until the 2017 publication of transformers."
  },
  {
    "index_id": 156,
    "doc_id": "transformers.txt",
    "chunk_id": 32,
    "text": "However, LSTM still used sequential processing, like most other RNNs.[note 2] Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. Modern transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window."
  },
  {
    "index_id": 157,
    "doc_id": "transformers.txt",
    "chunk_id": 33,
    "text": "The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input.[16] One of its two networks has \"fast weights\" or \"dynamic links\" (1981).[17][18][19] A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries.[16] This w"
  },
  {
    "index_id": 158,
    "doc_id": "transformers.txt",
    "chunk_id": 34,
    "text": "as later shown to be equivalent to the unnormalized linear transformer.[20][21]\n\nAttention with seq2seq\nMain article: Seq2seq § History\nThe idea of encoder–decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.[22][23]\n\nA 380M-parameter model for machine translation uses two long"
  },
  {
    "index_id": 159,
    "doc_id": "transformers.txt",
    "chunk_id": 35,
    "text": "short-term memories (LSTM).[23] Its architecture consists of two parts."
  },
  {
    "index_id": 160,
    "doc_id": "transformers.txt",
    "chunk_id": 36,
    "text": "The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens."
  },
  {
    "index_id": 161,
    "doc_id": "transformers.txt",
    "chunk_id": 37,
    "text": "Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM.[22] Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.[24][25]\n\nThese early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed."
  },
  {
    "index_id": 162,
    "doc_id": "transformers.txt",
    "chunk_id": 38,
    "text": "Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output."
  },
  {
    "index_id": 163,
    "doc_id": "transformers.txt",
    "chunk_id": 39,
    "text": "If the input is long, then the output vector would not be able to contain all relevant information, degrading the output."
  },
  {
    "index_id": 164,
    "doc_id": "transformers.txt",
    "chunk_id": 40,
    "text": "As evidence, reversing the input sentence improved seq2seq translation.[26]\n\nThe RNN search model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily."
  },
  {
    "index_id": 165,
    "doc_id": "transformers.txt",
    "chunk_id": 41,
    "text": "The name is because it \"emulates searching through a source sentence during decoding a translation\".[4]\n\nThe relative performances were compared between global (that of RNN search) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.[27]\n\nIn 2016, Goog"
  },
  {
    "index_id": 166,
    "doc_id": "transformers.txt",
    "chunk_id": 42,
    "text": "le Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation."
  },
  {
    "index_id": 167,
    "doc_id": "transformers.txt",
    "chunk_id": 43,
    "text": "The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM.[28] It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.[29]\n\nParallelizing attention\nMain article: Attention (machine learning) § History\nSeq2seq models with attention (including self-attention) still suffered from the same issue w"
  },
  {
    "index_id": 168,
    "doc_id": "transformers.txt",
    "chunk_id": 44,
    "text": "ith recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs."
  },
  {
    "index_id": 169,
    "doc_id": "transformers.txt",
    "chunk_id": 45,
    "text": "In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs.[30] One of its authors, Jakob Uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title \"attention is all you need\"."
  },
  {
    "index_id": 170,
    "doc_id": "transformers.txt",
    "chunk_id": 46,
    "text": "[31] That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical.[31] In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.[32]"
  },
  {
    "index_id": 171,
    "doc_id": "vector_databases.txt",
    "chunk_id": 0,
    "text": "Vector databases have grown in popularity thanks to their usefulness for Large Language Models (LLMs), Machine Learning (ML), and Artificial Intelligence (AI) applications. With a different approach than traditional structured and relational databases, vector databases excel at AI-friendly tasks like semantic search and recommendations."
  },
  {
    "index_id": 172,
    "doc_id": "vector_databases.txt",
    "chunk_id": 1,
    "text": "A vector database stores its data as high-dimensional vectors. These vectors help give data context and relation rather than only distinct attributes. By breaking down the elements of a vector database, you’ll understand how they can enhance your development and data pipelines. What is a vector?"
  },
  {
    "index_id": 173,
    "doc_id": "vector_databases.txt",
    "chunk_id": 2,
    "text": "A vector is a way to turn complex data (like words, images, or sounds) into a structured list of numbers, with each number capturing a specific feature. Think of it as a digital fingerprint that encodes an item’s unique qualities. Whether it’s a product image, a document, or user interaction, nearly anything can be turned into a vector to simplify processing and searching."
  },
  {
    "index_id": 174,
    "doc_id": "vector_databases.txt",
    "chunk_id": 3,
    "text": "This understanding and connecting of data attributes allows AI models to compare items based on similarity and make both broader and more nuanced conclusions. Vectors help make sense of unstructured data by finding relationships in meaning and context, critical for applications like recommendation systems and natural language processing."
  },
  {
    "index_id": 175,
    "doc_id": "vector_databases.txt",
    "chunk_id": 4,
    "text": "Which leads right into data storage – a vector database is a data store specialized for this kind of retrieval. What is a vector database? A vector database is built specifically for handling data represented as vectors: the lists of numbers that capture the important details of complex data like text, images, or video."
  },
  {
    "index_id": 176,
    "doc_id": "vector_databases.txt",
    "chunk_id": 5,
    "text": "These databases make it easier for machines to discover and understand related information based on meaning or similarity rather than just exact matches. This is a huge advantage for AI-powered systems, like recommendation engines or large language models (LLMs), which need to connect data to reflect context and subtle relationships."
  },
  {
    "index_id": 177,
    "doc_id": "vector_databases.txt",
    "chunk_id": 6,
    "text": "In addition to the concept of a vector itself, certain elements are foundational to vector databases: embeddings, similarity search, cosine similarity/distance metrics, and dimensionality reduction. What are embeddings? Embeddings are vectors, but they’re a type specially designed by machine learning models to capture complex relationships within data."
  },
  {
    "index_id": 178,
    "doc_id": "vector_databases.txt",
    "chunk_id": 7,
    "text": "Think of embeddings as compressed, context-rich representations of high-dimensional information. For example, in natural language processing, an embedding for a word or sentence encodes not just the word itself but also its relationships to other words based on meaning. This allows systems to perform similarity searches based on context, not just surface-level features."
  },
  {
    "index_id": 179,
    "doc_id": "vector_databases.txt",
    "chunk_id": 8,
    "text": "Embeddings are vectors optimized to capture the “essence” of data, making them foundational for AI tasks like search, recommendation, and language understanding. What is similarity search? This is where vector databases shine. Rather than looking for exact matches, they find data points that are similar to what’s being searched for based on their vectors."
  },
  {
    "index_id": 180,
    "doc_id": "vector_databases.txt",
    "chunk_id": 9,
    "text": "Instead of looking for identical matches, a similarity search finds entries that are “close” to the input based on certain features, like context or visual resemblance\n\nSo, if you search for an image of a shoe, the database will pull up other shoes, even if they don’t match exactly. This ability to retrieve contextually relevant items is crucial for AI-driven tasks like recommendation engines."
  },
  {
    "index_id": 181,
    "doc_id": "vector_databases.txt",
    "chunk_id": 10,
    "text": "What is cosine similarity (and other distance metrics)? Remember your trigonometry classes? Cosine similarity and other distance metrics are mathematical tools that measure how close two vectors are in high-dimensional space. These tools allow vector databases to quantify relationships, enabling precise results for similarity searches."
  },
  {
    "index_id": 182,
    "doc_id": "vector_databases.txt",
    "chunk_id": 11,
    "text": "Think of them as the numerical basis for deciding whether two items are meaningfully related. Cosine similarity calculates the angle between vectors to determine their similarity, as if they were making one angle of a triangle. Vectors pointing in similar directions have a higher cosine similarity score."
  },
  {
    "index_id": 183,
    "doc_id": "vector_databases.txt",
    "chunk_id": 12,
    "text": "Additional metrics include:\n\nEuclidean distance, measuring the straight-line distance between two vectors, commonly used for continuous data thathelps capture thosenuanced relationships\nManhattan distance, calculating the sum of absolute differences across dimensions, suitable for grid-based data\nHamming distance, counting the differing elements, often used with binary or categorical data\nJaccard"
  },
  {
    "index_id": 184,
    "doc_id": "vector_databases.txt",
    "chunk_id": 13,
    "text": "similarity, measuring similarity between two sets, ideal for sparse data like document word frequencies\nMinkowski distance, which flexes to generalize Euclidean and Manhattan distances, adjustable for various data types\nThese distance metrics enable vector databases to efficiently match related data points."
  },
  {
    "index_id": 185,
    "doc_id": "vector_databases.txt",
    "chunk_id": 14,
    "text": "But as vectors capture more features, they grow in size and complexity, which can slow down searches. That’s where dimensionality reduction comes in. What is dimensionality reduction? Data can have many dimensions (think features or characteristics)."
  },
  {
    "index_id": 186,
    "doc_id": "vector_databases.txt",
    "chunk_id": 15,
    "text": "To speed things up and make the data easier to work with, dimensionality reduction reduce the number of features without losing the most important ones. Approaches like principal component analysis (PCA), t-SNE, and others (below) compress data into fewer dimensions, maintaining its core characteristics."
  },
  {
    "index_id": 187,
    "doc_id": "vector_databases.txt",
    "chunk_id": 16,
    "text": "This helps vector databases perform similarity searches faster and more efficiently, especially as datasets grow larger. t-SNE (t-distributed Stochastic Neighbor Embedding) projects high-dimensional data into two or three dimensions for visualization. It preserves the relative distances of similar data points, making it easier to see clusters and patterns in complex datasets."
  },
  {
    "index_id": 188,
    "doc_id": "vector_databases.txt",
    "chunk_id": 17,
    "text": "PCA (Principal Component Analysis) identifies the main directions where data varies the most. PCA transforms the data to align along these components, reducing the number of features while retaining as much information as possible. By looking at the most significant differences, PCA makes data more manageable for pattern recognition, visualization, and speeding up machine learning models."
  },
  {
    "index_id": 189,
    "doc_id": "vector_databases.txt",
    "chunk_id": 18,
    "text": "UMAP (Uniform Manifold Approximation and Projection) reduces data into two or three dimensions for visualization, focusing on both local and global data structure. It’s faster than t-SNE and retains more of the overall data shape, making it ideal for larger datasets where structure matters. Autoencoders are neural networks designed to compress and then reconstruct data."
  },
  {
    "index_id": 190,
    "doc_id": "vector_databases.txt",
    "chunk_id": 19,
    "text": "By learning to represent data efficiently, autoencoders capture complex, nonlinear relationships and are well-suited for very high-dimensional data. LDA (Linear Discriminant Analysis) reduces dimensions by finding directions that best separate predefined classes. It’s often used in supervised learning, where the goal is to enhance class separation for better model performance."
  },
  {
    "index_id": 191,
    "doc_id": "vector_databases.txt",
    "chunk_id": 20,
    "text": "When these concepts come together, they help vector databases do their best: find, compare, and retrieve complex information quickly and efficiently – especially compared to traditional databases."
  },
  {
    "index_id": 192,
    "doc_id": "vector_databases.txt",
    "chunk_id": 21,
    "text": "Vector databases vs traditional relational databases\nTraditional relational databases (like MySQL or PostgreSQL) are great when storing and retrieving structured data with clear relationships, such as rows and columns of customer orders or inventory."
  },
  {
    "index_id": 193,
    "doc_id": "vector_databases.txt",
    "chunk_id": 22,
    "text": "However, when AI-driven applications need to analyze vast amounts of unstructured data (like text, images, or videos), relational databases fall short. They aren’t optimized for tasks requiring comparisons based on patterns or context—essential for AI applications like natural language processing and recommendation systems. Vector databases, by contrast, are built for these modern AI tasks."
  },
  {
    "index_id": 194,
    "doc_id": "vector_databases.txt",
    "chunk_id": 23,
    "text": "They store data as vectors that capture complex relationships, making it easy to search by meaning, similarity, or context rather than exact matches. This makes them a better fit for AI workloads like semantic search, personalized recommendations, and context-driven analytics, where interpreting meaning and nuance is key."
  },
  {
    "index_id": 195,
    "doc_id": "vector_databases.txt",
    "chunk_id": 24,
    "text": "Purpose-built for the complexities of AI/ML a vector databases works by reading between the rows and columns, so to speak. How do vector databases work? Vector databases convert complex data like text, images, or other unstructured information into vectors, essentially lists of numbers. These vectors represent key features of the data in a high-dimensional space."
  },
  {
    "index_id": 196,
    "doc_id": "vector_databases.txt",
    "chunk_id": 25,
    "text": "Vector databases work in essentially two stages. The first is creating embeddings within the database. Creating embeddings\nWhen data enters a vector database, it’s passed through a machine-learning model that generates embeddings. These embeddings are vectors, where each dimension captures a particular data feature."
  },
  {
    "index_id": 197,
    "doc_id": "vector_databases.txt",
    "chunk_id": 26,
    "text": "For example, in natural language processing (NLP), words or sentences are converted into vectors based on their meanings, so similar words like “car” and “vehicle” end up with vectors close to each other in this space. Embeddings  allow the vector database to store relationships and similarities between data points rather than just the data itself."
  },
  {
    "index_id": 198,
    "doc_id": "vector_databases.txt",
    "chunk_id": 27,
    "text": "Storing vectors\nOnce embeddings are created, they’re stored as vectors in the database, each representing unique data features. Unlike traditional databases that organize data in rows and columns, a vector database stores these high-dimensional vectors, capturing relationships between data points."
  },
  {
    "index_id": 199,
    "doc_id": "vector_databases.txt",
    "chunk_id": 28,
    "text": "This structure allows quick access to similar items, making the data accessible for tasks that depend on contextual relevance. Querying vectors\nWhen a query is run, it too is – you guessed it – transformed into a vector that captures the search’s essential features."
  },
  {
    "index_id": 200,
    "doc_id": "vector_databases.txt",
    "chunk_id": 29,
    "text": "The vector database then uses similarity search algorithms — such as cosine similarity or Euclidean distance — to find vectors closest to the query. This enables the database to retrieve items based on meaning rather than exact matches. This capability is essential for applications like semantic search, where interpreting “closeness” in meaning or appearance is key."
  },
  {
    "index_id": 201,
    "doc_id": "vector_databases.txt",
    "chunk_id": 30,
    "text": "Implementing vector databases\nVector databases are a game-changer for businesses looking to scale their AI and machine learning projects. They’re designed to handle massive amounts of complex data, perfect for powering personalized recommendations, AI-driven search, and more."
  },
  {
    "index_id": 202,
    "doc_id": "vector_databases.txt",
    "chunk_id": 31,
    "text": "When deciding on a vector database, there are several options, including open-source solutions like Milvus, Pinecone, and Weaviate. Each has its strengths. Some databases excel in scalability, and others in ease of integration. The key is to choose one that aligns with your project needs, whether dealing with AI search, recommendations, or managing large-scale data sets."
  },
  {
    "index_id": 203,
    "doc_id": "vector_databases.txt",
    "chunk_id": 32,
    "text": "As teams integrate vector databases and other innovative environments to support AI/ML capabilities, the next big challenge is to automate and unite data pipeline change management to keep pace with constant structural enhancements and evolutions. What is a Vector Database?"
  },
  {
    "index_id": 204,
    "doc_id": "vector_databases.txt",
    "chunk_id": 33,
    "text": "A vector database indexes and stores vector embeddings for fast retrieval and similarity search, with capabilities like CRUD operations, metadata filtering, horizontal scaling, and serverless. We’re in the midst of the AI revolution. It’s upending any industry it touches, promising great innovations - but it also introduces new challenges."
  },
  {
    "index_id": 205,
    "doc_id": "vector_databases.txt",
    "chunk_id": 34,
    "text": "Efficient data processing has become more crucial than ever for applications that involve large language models, generative AI, and semantic search."
  },
  {
    "index_id": 206,
    "doc_id": "vector_databases.txt",
    "chunk_id": 35,
    "text": "All of these new applications rely on vector embeddings, a type of vector data representation that carries within it semantic information that’s critical for the AI to gain understanding and maintain a long-term memory they can draw upon when executing complex tasks."
  },
  {
    "index_id": 207,
    "doc_id": "vector_databases.txt",
    "chunk_id": 36,
    "text": "Embeddings are generated by AI models (such as Large Language Models) and have many attributes or features, making their representation challenging to manage. In the context of AI and machine learning, these features represent different dimensions of the data that are essential for understanding patterns, relationships, and underlying structures."
  },
  {
    "index_id": 208,
    "doc_id": "vector_databases.txt",
    "chunk_id": 37,
    "text": "That is why we need a specialized database designed specifically for handling this data type. Vector databases like Pinecone fulfill this requirement by offering optimized storage and querying capabilities for embeddings."
  },
  {
    "index_id": 209,
    "doc_id": "vector_databases.txt",
    "chunk_id": 38,
    "text": "Vector databases have the capabilities of a traditional database that are absent in standalone vector indexes and the specialization of dealing with vector embeddings, which traditional scalar-based databases lack."
  },
  {
    "index_id": 210,
    "doc_id": "vector_databases.txt",
    "chunk_id": 39,
    "text": "The challenge of working with vector data is that traditional scalar-based databases can’t keep up with the complexity and scale of such data, making it difficult to extract insights and perform real-time analysis."
  },
  {
    "index_id": 211,
    "doc_id": "vector_databases.txt",
    "chunk_id": 40,
    "text": "That’s where vector databases come into play – they are intentionally designed to handle this type of data and offer the performance, scalability, and flexibility you need to make the most out of your data. We are seeing the next generation of vector databases introduce more sophisticated architectures to handle the efficient cost and scaling of intelligence."
  },
  {
    "index_id": 212,
    "doc_id": "vector_databases.txt",
    "chunk_id": 41,
    "text": "This ability is handled by serverless vector databases, that can separate the cost of storage and compute to enable low-cost knowledge support for AI. With a vector database, we can add knowledge to our AIs, like semantic information retrieval, long-term memory, and more."
  },
  {
    "index_id": 213,
    "doc_id": "vector_databases.txt",
    "chunk_id": 42,
    "text": "The diagram below gives us a better understanding of the role of vector databases in this type of application:\n\nVector Database\nLet’s break this down:\n\nFirst, we use the embedding model to create vector embeddings for the content we want to index. The vector embedding is inserted into the vector database, with some reference to the original content the embedding was created from."
  },
  {
    "index_id": 214,
    "doc_id": "vector_databases.txt",
    "chunk_id": 43,
    "text": "When the application issues a query, we use the same embedding model to create embeddings for the query and use those embeddings to query the database for similar vector embeddings. As mentioned before, those similar embeddings are associated with the original content that was used to create them. What’s the difference between a vector index and a vector database?"
  },
  {
    "index_id": 215,
    "doc_id": "vector_databases.txt",
    "chunk_id": 44,
    "text": "Standalone vector indices like FAISS (Facebook AI Similarity Search) can significantly improve the search and retrieval of vector embeddings, but they lack capabilities that exist in any database. Vector databases, on the other hand, are purpose-built to manage vector embeddings, providing several advantages over using standalone vector indices:"
  }
]